{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "            Conv2d-3         [-1, 64, 224, 224]           4,096\n",
      "       BatchNorm2d-4         [-1, 64, 224, 224]             128\n",
      "            Conv2d-5         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 224, 224]             128\n",
      "            Conv2d-7        [-1, 256, 224, 224]          16,384\n",
      "       BatchNorm2d-8        [-1, 256, 224, 224]             512\n",
      " AdaptiveAvgPool2d-9            [-1, 256, 1, 1]               0\n",
      "           Linear-10                   [-1, 16]           4,096\n",
      "             ReLU-11                   [-1, 16]               0\n",
      "           Linear-12                  [-1, 256]           4,096\n",
      "          Sigmoid-13                  [-1, 256]               0\n",
      "         SE_Block-14        [-1, 256, 224, 224]               0\n",
      "           Conv2d-15        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-16        [-1, 256, 224, 224]             512\n",
      "       Bottleneck-17        [-1, 256, 224, 224]               0\n",
      "           Conv2d-18         [-1, 64, 224, 224]          16,384\n",
      "      BatchNorm2d-19         [-1, 64, 224, 224]             128\n",
      "           Conv2d-20         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-21         [-1, 64, 224, 224]             128\n",
      "           Conv2d-22        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-23        [-1, 256, 224, 224]             512\n",
      "AdaptiveAvgPool2d-24            [-1, 256, 1, 1]               0\n",
      "           Linear-25                   [-1, 16]           4,096\n",
      "             ReLU-26                   [-1, 16]               0\n",
      "           Linear-27                  [-1, 256]           4,096\n",
      "          Sigmoid-28                  [-1, 256]               0\n",
      "         SE_Block-29        [-1, 256, 224, 224]               0\n",
      "       Bottleneck-30        [-1, 256, 224, 224]               0\n",
      "           Conv2d-31         [-1, 64, 224, 224]          16,384\n",
      "      BatchNorm2d-32         [-1, 64, 224, 224]             128\n",
      "           Conv2d-33         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-34         [-1, 64, 224, 224]             128\n",
      "           Conv2d-35        [-1, 256, 224, 224]          16,384\n",
      "      BatchNorm2d-36        [-1, 256, 224, 224]             512\n",
      "AdaptiveAvgPool2d-37            [-1, 256, 1, 1]               0\n",
      "           Linear-38                   [-1, 16]           4,096\n",
      "             ReLU-39                   [-1, 16]               0\n",
      "           Linear-40                  [-1, 256]           4,096\n",
      "          Sigmoid-41                  [-1, 256]               0\n",
      "         SE_Block-42        [-1, 256, 224, 224]               0\n",
      "       Bottleneck-43        [-1, 256, 224, 224]               0\n",
      "           Conv2d-44        [-1, 128, 224, 224]          32,768\n",
      "      BatchNorm2d-45        [-1, 128, 224, 224]             256\n",
      "           Conv2d-46        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-47        [-1, 128, 112, 112]             256\n",
      "           Conv2d-48        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-49        [-1, 512, 112, 112]           1,024\n",
      "AdaptiveAvgPool2d-50            [-1, 512, 1, 1]               0\n",
      "           Linear-51                   [-1, 32]          16,384\n",
      "             ReLU-52                   [-1, 32]               0\n",
      "           Linear-53                  [-1, 512]          16,384\n",
      "          Sigmoid-54                  [-1, 512]               0\n",
      "         SE_Block-55        [-1, 512, 112, 112]               0\n",
      "           Conv2d-56        [-1, 512, 112, 112]         131,072\n",
      "      BatchNorm2d-57        [-1, 512, 112, 112]           1,024\n",
      "       Bottleneck-58        [-1, 512, 112, 112]               0\n",
      "           Conv2d-59        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-60        [-1, 128, 112, 112]             256\n",
      "           Conv2d-61        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-62        [-1, 128, 112, 112]             256\n",
      "           Conv2d-63        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-64        [-1, 512, 112, 112]           1,024\n",
      "AdaptiveAvgPool2d-65            [-1, 512, 1, 1]               0\n",
      "           Linear-66                   [-1, 32]          16,384\n",
      "             ReLU-67                   [-1, 32]               0\n",
      "           Linear-68                  [-1, 512]          16,384\n",
      "          Sigmoid-69                  [-1, 512]               0\n",
      "         SE_Block-70        [-1, 512, 112, 112]               0\n",
      "       Bottleneck-71        [-1, 512, 112, 112]               0\n",
      "           Conv2d-72        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-73        [-1, 128, 112, 112]             256\n",
      "           Conv2d-74        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-75        [-1, 128, 112, 112]             256\n",
      "           Conv2d-76        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-77        [-1, 512, 112, 112]           1,024\n",
      "AdaptiveAvgPool2d-78            [-1, 512, 1, 1]               0\n",
      "           Linear-79                   [-1, 32]          16,384\n",
      "             ReLU-80                   [-1, 32]               0\n",
      "           Linear-81                  [-1, 512]          16,384\n",
      "          Sigmoid-82                  [-1, 512]               0\n",
      "         SE_Block-83        [-1, 512, 112, 112]               0\n",
      "       Bottleneck-84        [-1, 512, 112, 112]               0\n",
      "           Conv2d-85        [-1, 128, 112, 112]          65,536\n",
      "      BatchNorm2d-86        [-1, 128, 112, 112]             256\n",
      "           Conv2d-87        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-88        [-1, 128, 112, 112]             256\n",
      "           Conv2d-89        [-1, 512, 112, 112]          65,536\n",
      "      BatchNorm2d-90        [-1, 512, 112, 112]           1,024\n",
      "AdaptiveAvgPool2d-91            [-1, 512, 1, 1]               0\n",
      "           Linear-92                   [-1, 32]          16,384\n",
      "             ReLU-93                   [-1, 32]               0\n",
      "           Linear-94                  [-1, 512]          16,384\n",
      "          Sigmoid-95                  [-1, 512]               0\n",
      "         SE_Block-96        [-1, 512, 112, 112]               0\n",
      "       Bottleneck-97        [-1, 512, 112, 112]               0\n",
      "           Conv2d-98        [-1, 256, 112, 112]         131,072\n",
      "      BatchNorm2d-99        [-1, 256, 112, 112]             512\n",
      "          Conv2d-100          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-101          [-1, 256, 56, 56]             512\n",
      "          Conv2d-102         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-103         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-104           [-1, 1024, 1, 1]               0\n",
      "          Linear-105                   [-1, 64]          65,536\n",
      "            ReLU-106                   [-1, 64]               0\n",
      "          Linear-107                 [-1, 1024]          65,536\n",
      "         Sigmoid-108                 [-1, 1024]               0\n",
      "        SE_Block-109         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-110         [-1, 1024, 56, 56]         524,288\n",
      "     BatchNorm2d-111         [-1, 1024, 56, 56]           2,048\n",
      "      Bottleneck-112         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-113          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-114          [-1, 256, 56, 56]             512\n",
      "          Conv2d-115          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-116          [-1, 256, 56, 56]             512\n",
      "          Conv2d-117         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-119           [-1, 1024, 1, 1]               0\n",
      "          Linear-120                   [-1, 64]          65,536\n",
      "            ReLU-121                   [-1, 64]               0\n",
      "          Linear-122                 [-1, 1024]          65,536\n",
      "         Sigmoid-123                 [-1, 1024]               0\n",
      "        SE_Block-124         [-1, 1024, 56, 56]               0\n",
      "      Bottleneck-125         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-126          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-127          [-1, 256, 56, 56]             512\n",
      "          Conv2d-128          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-129          [-1, 256, 56, 56]             512\n",
      "          Conv2d-130         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-131         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-132           [-1, 1024, 1, 1]               0\n",
      "          Linear-133                   [-1, 64]          65,536\n",
      "            ReLU-134                   [-1, 64]               0\n",
      "          Linear-135                 [-1, 1024]          65,536\n",
      "         Sigmoid-136                 [-1, 1024]               0\n",
      "        SE_Block-137         [-1, 1024, 56, 56]               0\n",
      "      Bottleneck-138         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-139          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-140          [-1, 256, 56, 56]             512\n",
      "          Conv2d-141          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-142          [-1, 256, 56, 56]             512\n",
      "          Conv2d-143         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-144         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-145           [-1, 1024, 1, 1]               0\n",
      "          Linear-146                   [-1, 64]          65,536\n",
      "            ReLU-147                   [-1, 64]               0\n",
      "          Linear-148                 [-1, 1024]          65,536\n",
      "         Sigmoid-149                 [-1, 1024]               0\n",
      "        SE_Block-150         [-1, 1024, 56, 56]               0\n",
      "      Bottleneck-151         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-152          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-153          [-1, 256, 56, 56]             512\n",
      "          Conv2d-154          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 56, 56]             512\n",
      "          Conv2d-156         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-157         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-158           [-1, 1024, 1, 1]               0\n",
      "          Linear-159                   [-1, 64]          65,536\n",
      "            ReLU-160                   [-1, 64]               0\n",
      "          Linear-161                 [-1, 1024]          65,536\n",
      "         Sigmoid-162                 [-1, 1024]               0\n",
      "        SE_Block-163         [-1, 1024, 56, 56]               0\n",
      "      Bottleneck-164         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-165          [-1, 256, 56, 56]         262,144\n",
      "     BatchNorm2d-166          [-1, 256, 56, 56]             512\n",
      "          Conv2d-167          [-1, 256, 56, 56]         589,824\n",
      "     BatchNorm2d-168          [-1, 256, 56, 56]             512\n",
      "          Conv2d-169         [-1, 1024, 56, 56]         262,144\n",
      "     BatchNorm2d-170         [-1, 1024, 56, 56]           2,048\n",
      "AdaptiveAvgPool2d-171           [-1, 1024, 1, 1]               0\n",
      "          Linear-172                   [-1, 64]          65,536\n",
      "            ReLU-173                   [-1, 64]               0\n",
      "          Linear-174                 [-1, 1024]          65,536\n",
      "         Sigmoid-175                 [-1, 1024]               0\n",
      "        SE_Block-176         [-1, 1024, 56, 56]               0\n",
      "      Bottleneck-177         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-178          [-1, 512, 56, 56]         524,288\n",
      "     BatchNorm2d-179          [-1, 512, 56, 56]           1,024\n",
      "          Conv2d-180          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-181          [-1, 512, 28, 28]           1,024\n",
      "          Conv2d-182         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-183         [-1, 2048, 28, 28]           4,096\n",
      "AdaptiveAvgPool2d-184           [-1, 2048, 1, 1]               0\n",
      "          Linear-185                  [-1, 128]         262,144\n",
      "            ReLU-186                  [-1, 128]               0\n",
      "          Linear-187                 [-1, 2048]         262,144\n",
      "         Sigmoid-188                 [-1, 2048]               0\n",
      "        SE_Block-189         [-1, 2048, 28, 28]               0\n",
      "          Conv2d-190         [-1, 2048, 28, 28]       2,097,152\n",
      "     BatchNorm2d-191         [-1, 2048, 28, 28]           4,096\n",
      "      Bottleneck-192         [-1, 2048, 28, 28]               0\n",
      "          Conv2d-193          [-1, 512, 28, 28]       1,048,576\n",
      "     BatchNorm2d-194          [-1, 512, 28, 28]           1,024\n",
      "          Conv2d-195          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-196          [-1, 512, 28, 28]           1,024\n",
      "          Conv2d-197         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-198         [-1, 2048, 28, 28]           4,096\n",
      "AdaptiveAvgPool2d-199           [-1, 2048, 1, 1]               0\n",
      "          Linear-200                  [-1, 128]         262,144\n",
      "            ReLU-201                  [-1, 128]               0\n",
      "          Linear-202                 [-1, 2048]         262,144\n",
      "         Sigmoid-203                 [-1, 2048]               0\n",
      "        SE_Block-204         [-1, 2048, 28, 28]               0\n",
      "      Bottleneck-205         [-1, 2048, 28, 28]               0\n",
      "          Conv2d-206          [-1, 512, 28, 28]       1,048,576\n",
      "     BatchNorm2d-207          [-1, 512, 28, 28]           1,024\n",
      "          Conv2d-208          [-1, 512, 28, 28]       2,359,296\n",
      "     BatchNorm2d-209          [-1, 512, 28, 28]           1,024\n",
      "          Conv2d-210         [-1, 2048, 28, 28]       1,048,576\n",
      "     BatchNorm2d-211         [-1, 2048, 28, 28]           4,096\n",
      "AdaptiveAvgPool2d-212           [-1, 2048, 1, 1]               0\n",
      "          Linear-213                  [-1, 128]         262,144\n",
      "            ReLU-214                  [-1, 128]               0\n",
      "          Linear-215                 [-1, 2048]         262,144\n",
      "         Sigmoid-216                 [-1, 2048]               0\n",
      "        SE_Block-217         [-1, 2048, 28, 28]               0\n",
      "      Bottleneck-218         [-1, 2048, 28, 28]               0\n",
      "AdaptiveAvgPool2d-219           [-1, 2048, 1, 1]               0\n",
      "          Linear-220                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 26,035,786\n",
      "Trainable params: 26,035,786\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3914.25\n",
      "Params size (MB): 99.32\n",
      "Estimated Total Size (MB): 4014.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    " \n",
    "'''-------------SE模块-----------------------------'''\n",
    "#全局平均池化+1*1卷积核+ReLu+1*1卷积核+Sigmoid\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, inchannel, ratio=16):\n",
    "        super(SE_Block, self).__init__()\n",
    "        # 全局平均池化(Fsq操作)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # 两个全连接层(Fex操作)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(inchannel, inchannel // ratio, bias=False),  # 从 c -> c/r\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(inchannel // ratio, inchannel, bias=False),  # 从 c/r -> c\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "            # 读取批数据图片数量及通道数\n",
    "            b, c, h, w = x.size()\n",
    "            # Fsq操作：经池化后输出b*c的矩阵\n",
    "            y = self.gap(x).view(b, c)\n",
    "            # Fex操作：经全连接层输出（b，c，1，1）矩阵\n",
    "            y = self.fc(y).view(b, c, 1, 1)\n",
    "            # Fscale操作：将得到的权重乘以原来的特征图x\n",
    "            return x * y.expand_as(x)\n",
    " \n",
    "'''-------------（18-layer、34-layer）BasicBlock模块-----------------------------'''\n",
    "# residual block 结构\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
    "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
    "        # SE_Block放在BN之后，shortcut之前\n",
    "        self.SE = SE_Block(outchannel)\n",
    " \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != self.expansion*outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, self.expansion*outchannel,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            )\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        SE_out = self.SE(out)\n",
    "        out = out * SE_out\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    " \n",
    "'''-------------（50-layer、101-layer、152-layer）Bottleneck模块-----------------------------'''\n",
    "# residual block 结构\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    " \n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
    "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
    "        self.conv3 = nn.Conv2d(outchannel, self.expansion*outchannel,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*outchannel)\n",
    "        # SE_Block放在BN之后，shortcut之前\n",
    "        self.SE = SE_Block(self.expansion*outchannel)\n",
    " \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inchannel != self.expansion*outchannel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(inchannel, self.expansion*outchannel,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*outchannel)\n",
    "            )\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        SE_out = self.SE(out)\n",
    "        out = out * SE_out\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    " \n",
    "'''-------------搭建SE_ResNet结构-----------------------------'''\n",
    "class SE_ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(SE_ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    " \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)                  # conv1\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)       # conv2_x\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)      # conv3_x\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)      # conv4_x\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)      # conv5_x\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    " \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    " \n",
    " \n",
    "def SE_ResNet18():\n",
    "    return SE_ResNet(BasicBlock, [2, 2, 2, 2])\n",
    " \n",
    " \n",
    "def SE_ResNet34():\n",
    "    return SE_ResNet(BasicBlock, [3, 4, 6, 3])\n",
    " \n",
    " \n",
    "def SE_ResNet50():\n",
    "    return SE_ResNet(Bottleneck, [3, 4, 6, 3])\n",
    " \n",
    " \n",
    "def SE_ResNet101():\n",
    "    return SE_ResNet(Bottleneck, [3, 4, 23, 3])\n",
    " \n",
    " \n",
    "def SE_ResNet152():\n",
    "    return SE_ResNet(Bottleneck, [3, 8, 36, 3])\n",
    " \n",
    " \n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    model = SE_ResNet50()\n",
    "    print(model)\n",
    "    input = torch.randn(1, 3, 224, 224)\n",
    "    out = model(input)\n",
    "    print(out.shape)\n",
    "# test()\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = SE_ResNet50().to(device)\n",
    "    # 打印网络结构和参数\n",
    "    summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/qq_45981086/article/details/130869929\n",
    "\n",
    "![本地图片](../../img/SE.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
